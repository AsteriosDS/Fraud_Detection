{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ba632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2099d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc5d1a4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_fraud = cred[cred['Class']==0]\n",
    "fraud = cred[cred['Class']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa33a4",
   "metadata": {},
   "source": [
    "### Visualise the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of t-SNE\n",
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0eb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.concat([no_fraud.sample(3000),fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89293fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = plot_data.drop('Class', axis=1)\n",
    "y = plot_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfe8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the reduced feature vectors\n",
    "plt.figure(figsize=(7, 5))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "plt.title('t-SNE Visualization of Fraudulent and Non-Fraudulent Cases')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Customize legend labels\n",
    "legend_labels = ['Non-Fraud', 'Fraud']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels)\n",
    "plt.savefig('b4.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3672cbd",
   "metadata": {},
   "source": [
    "### 2/3 of non frauds are reserved for train/test and 1/3 of non frauds + frauds for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split non-fraudulent cases into train/test and validation sets\n",
    "train_test_no_fraud, val_no_fraud = train_test_split(\n",
    "    no_fraud, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Combine train/test non-fraudulent cases with fraudulent cases for the validation set\n",
    "val = pd.concat([val_no_fraud, fraud]).reset_index(drop=True)\n",
    "\n",
    "X_val = val.drop('Class',axis=1)\n",
    "y_val = val['Class']\n",
    "\n",
    "# create train and test sets\n",
    "X_train_test = train_test_no_fraud.drop('Class', axis=1)\n",
    "y_train_test = train_test_no_fraud['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_test, y_train_test, test_size=0.10, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42235c",
   "metadata": {},
   "source": [
    "### Normalisation to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35502ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe58b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformation to the test and val data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save val set for streamlit viz\n",
    "val = pd.DataFrame(X_val_scaled).merge(y_val,right_index=True, left_index=True)\n",
    "val_st = pd.concat([val[val['Class']==0].sample(50),val[val['Class']==1].sample(50)])\n",
    "val_st.drop('Class',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_st.to_csv('val_st.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f37e7",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    num_units_1 = trial.suggest_int('num_units_1', 50, 200)\n",
    "    num_units_2 = trial.suggest_int('num_units_2', 20, 100)\n",
    "\n",
    "    # Define the model architecture\n",
    "    input_layer = Input(shape=(X_train_scaled.shape[1], ))\n",
    "    encoded = Dense(num_units_1, activation='tanh')(input_layer)\n",
    "    encoded = Dense(num_units_2, activation='relu')(encoded)\n",
    "    decoded = Dense(num_units_2, activation='tanh')(encoded)\n",
    "    decoded = Dense(num_units_1, activation='tanh')(decoded)\n",
    "    output_layer = Dense(X_train_scaled.shape[1], activation='relu')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # Fit the model with early stopping based on validation loss\n",
    "    autoencoder.fit(X_train_scaled,\n",
    "                    X_train_scaled,\n",
    "                    batch_size=256,\n",
    "                    epochs=10,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.20,\n",
    "                    callbacks=[KerasPruningCallback(trial, 'val_loss')])\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss = autoencoder.evaluate(X_val_scaled, X_val_scaled)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding loss\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "\n",
    "print('Best Hyperparameters:', best_params)\n",
    "print('Best Loss:', best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters from the Optuna study\n",
    "best_params = study.best_params\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "input_layer = Input(shape=(X_train_scaled.shape[1], ))\n",
    "encoded = Dense(best_params['num_units_1'], activation='tanh')(input_layer)\n",
    "encoded = Dense(best_params['num_units_2'], activation='relu')(encoded)\n",
    "decoded = Dense(best_params['num_units_2'], activation='tanh')(encoded)\n",
    "decoded = Dense(best_params['num_units_1'], activation='tanh')(decoded)\n",
    "output_layer = Dense(X_train_scaled.shape[1], activation='relu')(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, output_layer)\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=best_params['learning_rate']), loss='mse')\n",
    "\n",
    "# Train the model on the entire training data\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, batch_size=256, epochs=10, shuffle=True)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss = round(autoencoder.evaluate(X_test_scaled, X_test_scaled),2)\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1d55d",
   "metadata": {},
   "source": [
    "## Optimizing Threshold K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8705d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the validation set using the trained autoencoder\n",
    "val_reconstructed = autoencoder.predict(X_val_scaled)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean(np.square(X_val_scaled - val_reconstructed), axis=1)\n",
    "\n",
    "# Initialize the first threshold as mean plus standard deviation\n",
    "initial_threshold = np.mean(reconstruction_errors) + np.std(reconstruction_errors)\n",
    "\n",
    "# Define a range of threshold values to try\n",
    "thresholds = np.arange(initial_threshold, 1.0, 0.05)\n",
    "\n",
    "best_threshold = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate through different threshold values\n",
    "for threshold in thresholds:\n",
    "    # Classify data points as normal or anomalous based on the threshold\n",
    "    predictions = (reconstruction_errors > threshold).astype(int)\n",
    "    \n",
    "    # Calculate prediction accuracy\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    \n",
    "    # Check if the current threshold gives better accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and accuracy\n",
    "print(\"Best Threshold:\", best_threshold)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26003d1e",
   "metadata": {},
   "source": [
    "## Latent Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "la_reps = Sequential()\n",
    "la_reps.add(autoencoder.layers[0])\n",
    "la_reps.add(autoencoder.layers[1])\n",
    "la_reps.add(autoencoder.layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = la_reps.predict(scaler.transform(plot_data.drop('Class', axis=1)))\n",
    "rep_y1 = np.zeros(plot_data[plot_data['Class'] == 0].shape[0])\n",
    "rep_y2 = np.ones(plot_data[plot_data['Class'] == 1].shape[0])\n",
    "rep_y = np.append(rep_y1, rep_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc055aae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = tsne.fit_transform(X)\n",
    "\n",
    "# Create a scatter plot of the reduced feature vectors\n",
    "plt.figure(figsize=(7, 5))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=rep_y, cmap='coolwarm')\n",
    "plt.title('t-SNE Visualization of Reconstructed Fraudulent and Non-Fraudulent Cases')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Customize legend labels\n",
    "legend_labels = ['Non-Fraud', 'Fraud']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels)\n",
    "plt.savefig('afterr.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec84d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the autoencoder model and weights\n",
    "autoencoder.save(\"autoencoder.h5\")\n",
    "autoencoder.save_weights(\"auto_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6707ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980b837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
